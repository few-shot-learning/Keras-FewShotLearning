{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from functools import partial\n",
    "from gpumonitor.callbacks.tf import TFGpuMonitorCallback\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, Flatten, GlobalMaxPooling2D, Input, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from keras_fsl.utils.datasets import assign, cache, cache_with_tf_record, read_decode_and_crop_jpeg, transform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Imports\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = tfds.load(name=\"cifar10\", split=\"train\")\n",
    "output_dir = Path(\"logs\") / \"benchmark_caching_performance\"\n",
    "(output_dir / \"cifar10\").mkdir(exist_ok=True, parents=True)\n",
    "examples = []\n",
    "for example in train_dataset:\n",
    "    tf.io.write_file(str(output_dir / \"cifar10\" / example[\"id\"].numpy().decode()), tf.io.encode_jpeg(example[\"image\"]))\n",
    "    examples += [{\"id\": example[\"id\"].numpy().decode(), \"label\": example[\"label\"].numpy()}]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Read Cifar10 dataset to dump images and create df\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    key: pd.DataFrame(examples)\n",
    "    .assign(filename=lambda df: str(output_dir / \"cifar10\") + \"/\" + df.id)\n",
    "    .pipe(lambda df: tf.data.Dataset.from_tensor_slices(df.to_dict(\"list\")))\n",
    "    .map(assign(image=read_decode_and_crop_jpeg), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .apply(cache_func)\n",
    "    .map(\n",
    "        transform(image=partial(tf.image.convert_image_dtype, dtype=tf.float32)),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    )\n",
    "    .map(\n",
    "        lambda x: (\n",
    "            tf.ensure_shape(x[\"image\"], train_dataset.element_spec[\"image\"].shape),\n",
    "            tf.ensure_shape(x[\"label\"], train_dataset.element_spec[\"label\"].shape),\n",
    "        ),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    )\n",
    "    .batch(64)\n",
    "    for key, cache_func in zip(\n",
    "        [\"tf_record_cache\", \"dataset_cache\", \"no_cache\"],\n",
    "        [cache_with_tf_record(output_dir / \"tf_record_cache\"), cache(output_dir / \"dataset_cache\"), lambda ds: ds],\n",
    "    )\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Create datasets\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Input(train_dataset.element_spec[\"image\"].shape),\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        MaxPooling2D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        MaxPooling2D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        MaxPooling2D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        MaxPooling2D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        GlobalMaxPooling2D(),\n",
    "        Flatten(),\n",
    "    ]\n",
    ")\n",
    "model.save_weights(str(output_dir / \"initial_weights.h5\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Create model\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for key, dataset in datasets.items():\n",
    "    model.load_weights(str(output_dir / \"initial_weights.h5\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "    model.fit(dataset, callbacks=[TFGpuMonitorCallback(delay=0.5)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Train\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}